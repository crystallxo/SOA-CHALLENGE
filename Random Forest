# Load necessary libraries
library(randomForest)
library(caret)
library(dplyr)
library(Metrics)
install.packages("")

data = read.csv("C:/Users/user/OneDrive/Documents/A UPH/SOA STUDY CASE/dam-clean (2) - data.csv")

# Median imputation for loss prop
loss_prop_clean <- na.omit(data$Loss_gf_Prop)
data$Loss_gf_Prop[is.na(data$Loss_gf_Prop)] <- median(loss_prop_clean, na.rm = TRUE)

# Median imputation for loss liab
loss_liab_clean <- na.omit(data$Loss_gf_Liab)
data$Loss_gf_Liab[is.na(data$Loss_gf_Liab)] <- median(loss_liab_clean, na.rm = TRUE)

missing_rows <- data[!complete.cases(data), ]  # Show only rows with NA
print(missing_rows)
# setelah di cek, udah gada yg missing values lg

# Convert categorical variables to factors
data$Region <- as.factor(data$Region)
data$Purpose <- as.factor(data$Purpose)
data$Hazard <- as.factor(data$Hazard)
data$Assessment <- as.factor(data$Assessment)

# Define the target variable (Loss Given Failure - Property)
target_variable <- "Total.loss"  # Adjust if needed

# Split data into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data[[target_variable]], p = 0.7, list = FALSE)
# createDataPartition() (from caret package) → Splits data randomly while ensuring class distribution remains similar.

train_data <- data[train_index, ]
test_data <- data[-train_index, ]

# Train the Random Forest model
set.seed(123)
p <- ncol(train_data) - 1  # Number of predictors
best_mtry <- floor(sqrt(p))  # Use sqrt(p) rule

predictors <- data %>%
  select(-c(Loss_gf_Prop, Loss_gf_Liab, Loss_gf_BI))

rf_model <- randomForest(as.formula(paste(target_variable, "~ .")), 
                         data = predictors, ntree = 100, mtry = best_mtry, importance=TRUE)

#rf_model <- randomForest(as.formula(paste(target_variable, "~ .")), data = train_data, ntree = 100, mtry = best_mtry, importance = TRUE)


# Evaluate model performance
rf_predictions <- predict(rf_model, newdata = test_data)

true_values <- test_data$Total.loss

# Compute regression metrics
mse <- mse(true_values, rf_predictions)
rmse <- rmse(true_values, rf_predictions)
mae <- mae(true_values, rf_predictions)
r2 <- cor(true_values, rf_predictions)^2  # R-squared

# Print results
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("MAE:", mae, "\n")
cat("R-squared:", r2, "\n")

####################################################################################

#compare hasil for testing and training, utk melihat ada overfitting ga
# Compute predictions on training data

train_data <- 

train_rf_predictions <- predict(rf_model, train_data)

# Compute RMSE and R² for training data
train_rmse <- rmse(train_data$Total.loss, train_rf_predictions)
train_r2 <- cor(train_data$Total.loss, train_rf_predictions)^2

# Print results
cat("Training RMSE:", train_rmse, "\n")
cat("Training R-squared:", train_r2, "\n")
cat("Test RMSE:", rmse, "\n")
cat("Test R-squared:", r2, "\n")

############################################

library(randomForest)

# Run automatic tuning for mtry (number of features per split)
best_mtry <- tuneRF(
  x = train_data[ , !names(train_data) %in% "Total.loss"],  # Exclude target variable
  y = train_data$Total.loss, 
  stepFactor = 1.5,  # Expands search range
  improve = 0.01,  # Stop if improvement is less than 1%
  trace = TRUE  # Show progress
)

print(best_mtry)  # Best mtry value

varImpPlot(rf_model)
